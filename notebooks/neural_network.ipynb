{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6034109c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d641397a",
   "metadata": {},
   "source": [
    "# Activation Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbac293",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        self.output = np.maximum(0, x)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.input <= 0] = 0\n",
    "        return self.dinputs\n",
    "\n",
    "\n",
    "class LReLU:\n",
    "    def __init__(self, alpha=0.01):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        self.output = np.where(x > 0, x, self.alpha * x)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = np.where(self.input > 0, dvalues, dvalues * self.alpha)\n",
    "        return self.dinputs\n",
    "\n",
    "\n",
    "class Sigmoid:\n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        self.output = 1 / (1 + np.exp(-x))\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues * (self.output * (1 - self.output))\n",
    "        return self.dinputs\n",
    "\n",
    "\n",
    "class Tanh:\n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        self.output = np.tanh(x)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues * (1 - self.output ** 2)\n",
    "        return self.dinputs\n",
    "\n",
    "\n",
    "class Softmax:\n",
    "    def forward(self, x):\n",
    "        exp_values = np.exp(x - np.max(x, axis=1, keepdims=True))  # stability trick\n",
    "        self.output = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        # True Softmax backward usually combined with cross-entropy loss\n",
    "        self.dinputs = dvalues.copy()\n",
    "        return self.dinputs\n",
    "\n",
    "\n",
    "class Linear:\n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        self.output = x\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        return self.dinputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14952081",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.array([[1, -2, 3], [-1, 2, -3]])\n",
    "relu = ReLU()\n",
    "lrelu"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
